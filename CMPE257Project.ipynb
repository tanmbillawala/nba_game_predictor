{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbd09ee-9206-4948-9ae6-e42e7d20976a",
   "metadata": {},
   "source": [
    "# NBA Game Predictor Model\n",
    "### CMPE 257 Project\n",
    "Authors: Kaushika Uppu, Miranda Billawala, Yun Ei Hlaing, Iris Cheung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f5cbd-6516-49cf-9271-94878f5f9d0c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030921a-827d-4a1f-af9b-104e618da8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28abb9c-228a-49bb-9721-215d0f183617",
   "metadata": {},
   "source": [
    "## NBA Game Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2dae76-eb39-4b8e-b8c2-9c1cc8d16cca",
   "metadata": {},
   "source": [
    "First, we load in all of the NBA game data from the CSV file. Exact code for gathering data is in a separate file and use the nba_api file. Only games from the 1985-1986 season and afterward are loaded in as the seasons before that are missing a very significant portion of the game statistics' data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f17e2e-851c-444d-bdb4-a1b63b594be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = pd.read_csv('all_game_stats.csv')\n",
    "all_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29527d-1036-4422-b002-364145da7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668ccae-04fb-491c-8618-84aa45589ff8",
   "metadata": {},
   "source": [
    "## Data Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffac81-fe5c-4b48-80bc-3bc14c9a6552",
   "metadata": {},
   "source": [
    "### Inputting Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078309b7-720e-4495-844c-5dd484470d3c",
   "metadata": {},
   "source": [
    "As shown below, there are a number of rows with the `SEASON_YEAR` variable missing. Therefore, we will calculate the `SEASON_YEAR` based on the `GAME_DATE` variable and fill in those missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de4b20-f959-4d54-a803-0413f59c2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['SEASON_YEAR'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448eefd-3ccd-4dbb-9046-e850c706ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in all_stats.iterrows():\n",
    "    if pd.isna(all_stats.iloc[index]['SEASON_YEAR']):\n",
    "        year_index = all_stats.iloc[index]['GAME_DATE'].find('-')\n",
    "        year = all_stats.iloc[index]['GAME_DATE'][:year_index]\n",
    "        month = all_stats.iloc[index]['GAME_DATE'][year_index+1:year_index+3]\n",
    "        if int(month) >= 10:\n",
    "            season = str(int(year)) + \"-\" + str(int(year)+1)[2:]\n",
    "        else:\n",
    "            season = str(int(year)-1) + \"-\" + str(int(year))[2:]\n",
    "        all_stats.loc[index, 'SEASON_YEAR'] = season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47202e49-459f-44a7-8492-12dc42f9a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_stats[all_stats['SEASON_YEAR'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef866a2-f702-44fd-a06a-f18be34384ba",
   "metadata": {},
   "source": [
    "Then, we convert the `SEASON_YEAR` variable into an integer variable of just the year that the season started (e.g., 1985 for '1985-86')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c687dd-20e4-4b8c-a13d-1c72a4954cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['SEASON_YEAR'] = all_stats['SEASON_YEAR'].str.split('-').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f615d-4836-4323-9e4d-31184a1c8ad6",
   "metadata": {},
   "source": [
    "A look at the new `SEASON_YEAR` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3f233-5d36-4683-a220-2437463c3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[['SEASON_YEAR']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90465e91-ccdf-4d0c-be3c-45034a4ed7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_stats.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f99f6d-4b06-43fb-8cd1-f0143907a212",
   "metadata": {},
   "source": [
    "As seen above, there are also 475 missing values in the `FG3_PCT` column. Taking a look at the `FG3A` column for the rows with missing values, we can see that they are all 0, hence why the `FG3_PCT` column has NaN values for these rows. Therefore, we filled the missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeda24-ddfd-4e93-aa08-098d498be239",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[all_stats['FG3_PCT'].isna()]['FG3A'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066d260-cd41-426f-bdbb-2d5e42c286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_indicies = all_stats[all_stats['FG3_PCT'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8b708-b9dd-4238-8609-974ab4eae720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in missing_indicies:\n",
    "    all_stats.loc[i, 'FG3_PCT'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a2a7d-9cef-473e-a156-72f6d87567d0",
   "metadata": {},
   "source": [
    "### Dropping Irrelevant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcc31a-833e-4283-a410-19539aee8777",
   "metadata": {},
   "source": [
    "There are a lot of other columns in the dataset that have a significant number of missing values. We will drop these columns, as most of them are also rankings for stats that are already in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4261ec-120d-4d70-b179-da7ef47220ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['SEASON_ID', 'GAME_ID', 'VIDEO_AVAILABLE', 'GP_RANK', 'W_RANK', 'L_RANK', 'W_PCT_RANK', 'MIN_RANK', 'FGM_RANK', 'FGA_RANK',\n",
    "           'FG_PCT_RANK', 'FG3M_RANK', 'FG3A_RANK', 'FG3_PCT_RANK', 'FTM_RANK', 'FTA_RANK', 'FT_PCT_RANK', 'OREB_RANK',\n",
    "           'DREB_RANK', 'REB_RANK', 'AST_RANK', 'TOV_RANK', 'STL_RANK', 'BLK_RANK', 'BLKA_RANK', 'PF_RANK', 'PFD_RANK',\n",
    "           'PTS_RANK', 'PLUS_MINUS_RANK', 'AVAILABLE_FLAG', 'BLKA', 'PFD' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8a959-e603-4256-a178-53a071b2a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned = all_stats.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863158e7-45ba-4e46-bbdc-05be2f1b440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a61ec3c-0882-45b4-8fc9-4babba0c9d3b",
   "metadata": {},
   "source": [
    "### Fixing Team ID\n",
    "Since Team ID seems to start at 1610612737, we are going to subtract this value from each `TEAM_ID` to get more readable numbers. And, we want to create a dictionary to hold team names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4ced6-270e-488a-a0c0-d23988d1f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned['TEAM_ID'] = [(all_stats_cleaned.iloc[i, 0]-1610612737) for i in range(all_stats_cleaned.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd87d05-8e5d-4d3d-aad2-08edea3e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad648f-8c27-463d-a5ca-e7a477c0cfd0",
   "metadata": {},
   "source": [
    "We then want to create a dictionary so we can determine ID from abbreviation and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe1672-1dad-4d27-b357-b46c781aac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_id_to_abb = {} # dictionary to convert from team_id to team_abbreviation\n",
    "team_abb_to_id = {} # dictionary to convert from team_abbreviation to team_id\n",
    "\n",
    "teams = (all_stats_cleaned[['TEAM_ID', 'TEAM_ABBREVIATION']]).drop_duplicates()\n",
    "\n",
    "for index, row in teams.iterrows() :\n",
    "    if row['TEAM_ID'] not in team_id_to_abb.keys():\n",
    "        team_id_to_abb[row['TEAM_ID']] = []\n",
    "    team_id_to_abb[row['TEAM_ID']].append(row['TEAM_ABBREVIATION'])\n",
    "    team_abb_to_id[row['TEAM_ABBREVIATION']] = row['TEAM_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7022359-af17-492e-bc78-2730de8b7c4c",
   "metadata": {},
   "source": [
    "### Cleaning Matchup Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b27ab2-10e4-467a-aab4-a5ae96afffb2",
   "metadata": {},
   "source": [
    "Next, the `MATCHUP` column contains information on the opponent as well as if it was a home or away game. To make sure these features are clear for the model, we split this information into two separate columns: `OPPONENT` and `HOME`. `HOME` is a binary variable where a value of 1 indicates a home game and a value of 0 indicates an away game. `OPPONENT` contains the team abbreviation of the other team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbfa16-5ccf-4e2a-bcf4-2db092046bc6",
   "metadata": {},
   "source": [
    "Creating `HOME` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db73d64-1359-47ce-bcc2-51a4d868cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_away = [0 if '@' in all_stats_cleaned['MATCHUP'].iloc[i] else 1 for i in range(len(all_stats_cleaned))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a0064-b978-468e-bd90-82d495e66428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_stats_cleaned.insert(5, 'HOME', home_away)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c9b55-bc54-4ac9-af47-4689b7b33f3c",
   "metadata": {},
   "source": [
    "Creating `OPPONENT` and `OPPONENT_ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f08650-34db-421c-9e13-a06c5db83bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opp = [all_stats_cleaned['MATCHUP'].iloc[i][-3:] for i in range(len(all_stats_cleaned))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9c80b-87b6-4a6f-9a05-c0aa2c19985c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_stats_cleaned.insert(6, 'OPPONENT', opp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea1ea2-81dc-4cea-b4a4-2e2ac4c6e9ca",
   "metadata": {},
   "source": [
    "Finally, we got rid of the `MATCHUP` column as it now contains redundant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc133b3-96e8-46ca-8da4-87798bf1e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned = all_stats_cleaned.drop(columns = ['MATCHUP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5c63a-fcbd-452a-94a7-c0327223d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218cd84-1cbe-4156-93f8-b806c2a3f774",
   "metadata": {},
   "source": [
    "### Cleaning up Game Date Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e215e-b008-4d06-82f7-810167113e49",
   "metadata": {},
   "source": [
    "In order for the model to interpret the date of the games, we decided to change the `GAME_DATE` column into datetime objects rather than keeping them as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301ecc5-f0f9-4cb4-918d-da18b78e5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned['GAME_DATE'] = pd.to_datetime(all_stats_cleaned['GAME_DATE'], yearfirst=True, format='ISO8601')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b16ee8-0a4c-4dff-9d01-b3116eb1368b",
   "metadata": {},
   "source": [
    "A look at the new `GAME_DATE` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70422f5-be8d-4c81-9cda-abe0b8e38f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned[['GAME_DATE']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085246d-a895-46b1-8310-be1cab13cced",
   "metadata": {},
   "source": [
    "### Cleaning up WL Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed2454-2d74-4678-a101-76fac858665e",
   "metadata": {},
   "source": [
    "The `WL` column states whether the team won or lost that specific game. However, we decided to convert this information into a binary variable `WIN`, which holds 1 for a win and 0 for a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9a1bc-03c0-4a75-80f1-099860d3d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = [1 if all_stats_cleaned.iloc[i]['WL'] == 'W' else 0 for i in range(len(all_stats_cleaned))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f108db3-9844-4c54-b789-3feea70f0f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_stats_cleaned.insert(6, 'WIN', win)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd81499-411c-4f3a-836b-828feecf3d19",
   "metadata": {},
   "source": [
    "Dropping `WL` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3212e6-4dd9-46e0-8f13-6cbaeaad77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned = all_stats_cleaned.drop(columns = ['WL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c2440-0fc3-44ea-babe-f25b484d2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8db0b-eb67-41f9-8287-676aeed18b2a",
   "metadata": {},
   "source": [
    "### Merging Home and Away Team Stats Into One Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c3044-4f7f-4516-8850-32da043a2f56",
   "metadata": {},
   "source": [
    "Currently, each game is represented by two separate rows in the dataset - one for the home team and one for the away team. To make the data more clear, we decided to combine the two rows into a single row with statistics for both teams. Since predicting with our model will pass one set order of team one and team two (i.e. Lakers as Team One, Warriors as Team Two), we want to make sure that the model realizes games with the Lakers as Team Two and Warriors as Team One are more similar than may appear by the data. To do this, we will duplicate the rows and flip the teams. Then, we will have each game listed twice with the teams flipped. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f167431-5cca-4d38-befc-a7e832eaebdc",
   "metadata": {},
   "source": [
    "Firstly, we split the dataset into two : home games and away games. Then, we performed a join on these two datasets, matching each home team with its corresponding opponent based on the same dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a9b9a-9667-4f37-be8c-27b1c74ef6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = all_stats_cleaned[all_stats_cleaned.HOME == 1]\n",
    "away = all_stats_cleaned[all_stats_cleaned.HOME == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8f695-c8b8-423d-b2f1-e6273b6e3284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_stats_home = pd.merge(home, away, \n",
    "                          left_on=['GAME_DATE', 'OPPONENT'], \n",
    "                          right_on=['GAME_DATE', 'TEAM_ABBREVIATION'],\n",
    "                          suffixes=('_ONE', '_TWO'))\n",
    "combined_stats_away = pd.merge(away, home, \n",
    "                          left_on=['GAME_DATE', 'OPPONENT'], \n",
    "                          right_on=['GAME_DATE', 'TEAM_ABBREVIATION'],\n",
    "                          suffixes=('_ONE', '_TWO'))\n",
    "\n",
    "combined_stats = pd.concat([combined_stats_home, combined_stats_away], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448b5f0-f44c-4957-945d-f57bf5fd5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cde485-a219-4de4-923f-3815f6bbdb2d",
   "metadata": {},
   "source": [
    "Comparing the number of rows in the combined dataset to the original shows that the dataset row have been reduced by half, as each game is now represented by a single row instead of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca0d93-c685-4a5d-a79b-e0db3b7eb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eabb67-1072-4935-8263-73ac3fdde789",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f864936-7a4d-4a65-bb75-293986e31250",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af888f5e-afa3-44f7-8a76-b3f4e12005ae",
   "metadata": {},
   "source": [
    "#### Dropping Duplicate Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6eadb-8c9e-4edd-bba7-267512d7ee49",
   "metadata": {},
   "source": [
    "After merging the rows, there are some columns that appear twice or are now unneccessary to the dataset. These columns include `MIN_ONE`/`MIN_TWO` (length of game in minutes), `SEASON_YEAR_ONE`/`SEASON_YEAR_TWO`, `OPPONENT_ONE` and `OPPONENT_TWO`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f79796-bfd6-4991-8b7e-d835e758d8ee",
   "metadata": {},
   "source": [
    "We first checked if the `MIN_ONE` and `MIN_TWO` for each row has the same values. As seen below, there are 24 games where the minutes differed slightly. However, since the difference did not seem to be significant, we decided to retain one column and rename it `MIN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d683d2-7e36-4979-9e74-54f7457ea220",
   "metadata": {},
   "outputs": [],
   "source": [
    "(combined_stats['MIN_ONE'] != combined_stats['MIN_TWO']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e20896-7388-4ad1-ae28-4557f980c77c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_stats[combined_stats['MIN_ONE'] != combined_stats['MIN_TWO']][['MIN_ONE','MIN_TWO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749d449-34c6-4964-af4e-9113a32a9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stats = combined_stats.drop(columns = ['MIN_TWO', 'OPPONENT_ONE', 'OPPONENT_TWO', 'SEASON_YEAR_ONE'])\n",
    "combined_stats.rename(columns={'MIN_ONE': 'MIN', 'SEASON_YEAR_TWO': 'SEASON_YEAR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2f7bd-dd46-403b-bed5-e609b9bbe861",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024aff9e-3860-4f85-a5af-a9abffa1d597",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "In this section, we take a look at the data to better understand the different features as well as any possible trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c52d1f-ce41-4be2-8e8c-5c48640aa84c",
   "metadata": {},
   "source": [
    "### Rolling Average vs. Actual Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a875ca9-6012-4d42-859d-8f86aec573dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingAvg(team_id, feature_name, rolling_window = 3, season = 0, plot = True) :\n",
    "    \"\"\"\n",
    "    This function takes a team and feature and calculate the rolling average (not including the game on a given date)\n",
    "    and the actual value of that feature on the day. It can visualize this comparison and returns the RMSE between the \n",
    "    rolling average and actual value.\n",
    "\n",
    "    Inputs:\n",
    "    team_id: integer from 0-29 representing a team (required)\n",
    "    feature_name: string of a column of integer values from the dataset (required)\n",
    "    rolling_window: how many days to average over (default = 3)\n",
    "    season: season we are looking at (defaults to most current season)\n",
    "    plot: whether or not to plot the function (default = True)\n",
    "\n",
    "    Output: RMSE between predicted value (rolling average) and actual value\n",
    "    \"\"\"\n",
    "    if (season < all_stats_cleaned['SEASON_YEAR'].min()) or (season > all_stats_cleaned['SEASON_YEAR'].max()) :\n",
    "        season = all_stats_cleaned['SEASON_YEAR'].max()\n",
    "        \n",
    "    data = all_stats_cleaned[(all_stats_cleaned['TEAM_ID'] == team_id) & (all_stats_cleaned['SEASON_YEAR'] == season)]\n",
    "    data = data[['GAME_DATE', feature_name]].sort_values(by='GAME_DATE')\n",
    "    data['SHIFTED'] = data[feature_name].shift(1)\n",
    "    data['ROLLING_AVG'] = data['SHIFTED'].rolling(window = rolling_window).mean()\n",
    "    if plot :\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(data['GAME_DATE'], data['ROLLING_AVG'], marker = 'o', linestyle = '-', label = \"Rolling Avg\")\n",
    "        plt.plot(data['GAME_DATE'], data[feature_name], marker = 'o', linestyle = '-', label = \"Actual Value\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Average Points\")\n",
    "        plt.title(\"Rolling Average Points Over Time\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    data = data.dropna()\n",
    "    error = np.sqrt(np.mean((data[feature_name].values-data['ROLLING_AVG'].values) ** 2))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bf3ac-c8f6-4f9c-9afa-859b89316828",
   "metadata": {},
   "source": [
    "The function above takes a team ID and a specific feature (with optional arguments of specifying the rolling window, season of interest, and to plot or not) and finds the rolling average. Our rolling average calculates a predicted value for the next game and the graph plots the prediction against the actual result. The function returns the error calculated by RMSE of a specific rolling window. We can use this to see how predictive previous games are of team performance in an upcoming game and decide what a good window might be.\n",
    "\n",
    "Below, we test rolling windows from 1 to 20 with the option to adjust the season, team, and feature. This can be used later on when building test examples for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511b476-4225-4b6a-9818-a04d3310ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = None\n",
    "best_window = 0\n",
    "team = 0\n",
    "feature = 'FGM'\n",
    "season = 2000\n",
    "\n",
    "for i in range(1, 20) :\n",
    "    rmse = rollingAvg(team, feature, i, season, False)\n",
    "    if min_error is None or rmse < min_error:\n",
    "        min_error = rmse\n",
    "        best_window = i\n",
    "\n",
    "print(\"Best Window:\", best_window)\n",
    "rollingAvg(team, feature, best_window, season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d2c4a-f427-450b-9268-02e2b837e642",
   "metadata": {},
   "source": [
    "The code above finds that the best window for the `FGM` variable is 11. It then visualizes the predictions agains the actual values and provides the RMSE of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c194ffe-13e6-45e0-83c1-993d12e64b03",
   "metadata": {},
   "source": [
    "### Team vs. Team Performance\n",
    "We look at a heatmap showing the win percentages between teams to see how teams perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dde456-f591-46b8-9091-87a758d860e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateWinMatrix(start_season = all_stats_cleaned['SEASON_YEAR'].min(), end_season = all_stats_cleaned['SEASON_YEAR'].max()) :\n",
    "    \"\"\"\n",
    "    This function takes a range of seasons and calculates the win percentages of the a team against all other teams for all games\n",
    "    occuring within the provided season. Each row represents the win percentages a team.\n",
    "\n",
    "    Inputs:\n",
    "    start_season: first season to look at (default first recorded season)\n",
    "    end_season: last season to consider (default most recent season)\n",
    "\n",
    "    Output: Numpy matrix containing win percentages for team by row.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_teams = len(team_id_to_abb)\n",
    "    np_win_matrix = np.zeros((num_teams, num_teams))\n",
    "    for team_one in range(num_teams):\n",
    "        for team_two, team_two_abb in team_id_to_abb.items() :\n",
    "            if team_one == team_two : continue\n",
    "            games = wins = 0\n",
    "            for x in team_two_abb :\n",
    "                matches = all_stats_cleaned[(all_stats_cleaned['TEAM_ID'] == team_one) & (all_stats_cleaned['OPPONENT'] == x) & (all_stats_cleaned['SEASON_YEAR'].between(start_season, end_season)) ]\n",
    "                games += len(matches)\n",
    "                wins += len(matches[matches['WIN'] == 1])\n",
    "        \n",
    "            np_win_matrix[team_one][team_two] = wins / games\n",
    "    return np_win_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ede3fc-1b68-4cc4-9b42-d825ae236c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_win_matrix = calculateWinMatrix(2020, 2025)\n",
    "teams = [team_id_to_abb[i][-1] for i in team_id_to_abb]\n",
    "plt.figure(figsize =(20,15))\n",
    "sns.heatmap(np_win_matrix, annot=True, cmap=\"coolwarm\", xticklabels=teams, yticklabels=teams) \n",
    "plt.xlabel(\"Opponent\")\n",
    "plt.ylabel(\"Team Win Percentages\")\n",
    "plt.title(\"Head-to-Head Win Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f187aa-2224-42eb-a237-6ae416fda3b6",
   "metadata": {},
   "source": [
    "Looking at the heatmap above, we can see that certain teams perform far better and worse than others. For example, DET loses more games than wins against nearly every team in the range from 2020 to 2025. The same goes for Washington. However, Washington seems to fare especially well against MIN despite MIN generally having win percentages above 50%. This tells us that Washington may perform especially well against MIN and increase their probability of winning despite generally losing their games. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10ce9b-c853-4f5a-9275-691f18d8e8d1",
   "metadata": {},
   "source": [
    "### Home Court Advantage\n",
    "Another important factor is home-court advantage (i.e. increased chance of winning due to playing at home). We want to observe how this affects teams. Since teams typically play half their games at home and half away (playing each team twice during a season, once at home at once away), we can forgo calculating home wins / total home games and away wins / total away games and instead just look at how many wins were home. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75164a0-e031-4340-95fa-7938445333ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homeWins(team_id) :\n",
    "    \"\"\"\n",
    "    This function takes a team ID and calculates the percentages of wins that are at home each season. \n",
    "    Then, we graph the values on a bar graph.\n",
    "\n",
    "    Inputs:\n",
    "    team_id: team ID, required\n",
    "    \"\"\"\n",
    "    start = all_stats_cleaned['SEASON_YEAR'].min()\n",
    "    end = all_stats_cleaned['SEASON_YEAR'].max()\n",
    "\n",
    "    years = list(range(start, end+1))\n",
    "    win_percentages = []\n",
    "    games = all_stats_cleaned[all_stats_cleaned['TEAM_ID'] == team_id]\n",
    "\n",
    "    for i in years :\n",
    "        games_in_season = games[games['SEASON_YEAR'] == i]\n",
    "        wins_in_season = games_in_season[games_in_season['WIN'] == 1]\n",
    "        home_wins = wins_in_season[wins_in_season['HOME'] == 1]\n",
    "        if len(wins_in_season) == 0 : \n",
    "            win_percentages.append(0)\n",
    "        else :\n",
    "            win_percentages.append(len(home_wins) / len(wins_in_season))\n",
    "\n",
    "    team_abb = team_id_to_abb[team_id][-1]\n",
    "    plt.bar(years, win_percentages)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Win Percentages At Home')\n",
    "    plt.title(f'Win Percentages at Home for {team_abb} over the Years')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c52f4-668e-4286-9eca-83521ed4dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeWins(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6b077-658f-47e4-8fd8-4471888829a3",
   "metadata": {},
   "source": [
    "The above bargraph shows us that for the Los Angeles Lakers (LAL), the game being at home results in a slightly higher probability of winning. Looking at more teams, we will see that this trend continues, although perhaps not as strong as some may think. This indicates the model may find whether the game is home or away to be a significant factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecddf8-6a56-4bc2-a0b7-e66ed44d386d",
   "metadata": {},
   "source": [
    "### Statistics in Games Won vs. Lost\n",
    "We also want to see how values compare in games they won or lost. We can do that by graphing the averages over a season for games a specific team lost and won as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7862df-5664-4761-874f-edbe5c805fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winLossAverages(team_id, feature) :\n",
    "    \"\"\"\n",
    "    This function takes a team ID and a feature and calculates the average value of that feature \n",
    "    for each season separated into games won or lost. This allows us to see how a value could be \n",
    "    used to predict if a team will win or not.\n",
    "    \"\"\"\n",
    "    start = all_stats_cleaned['SEASON_YEAR'].min()\n",
    "    end = all_stats_cleaned['SEASON_YEAR'].max()\n",
    "\n",
    "    years = list(range(start, end+1))\n",
    "    avg_for_wins = []\n",
    "    avg_for_losses = []\n",
    "\n",
    "    for y in years :\n",
    "        games = all_stats_cleaned[all_stats_cleaned['SEASON_YEAR'] == y]\n",
    "        wins = games[games['WIN'] == 1]\n",
    "        losses = games[games['WIN'] == 0]\n",
    "\n",
    "        avg_for_wins.append(wins[feature].mean())\n",
    "        avg_for_losses.append(losses[feature].mean())\n",
    "\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.plot(years, avg_for_wins, linestyle = '-', label = \"Games Won\")\n",
    "    plt.plot(years, avg_for_losses, linestyle = '-', label = \"Games Lost\")\n",
    "    plt.xlabel(\"Seasons\")\n",
    "    plt.ylabel(f\"Average {feature}\")\n",
    "    plt.title(f\"Comparing Averages of {feature} For Games Won or Lost by {team_id_to_abb[team_id][-1]}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21312e32-512c-44e2-aa87-1e7ade078d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "winLossAverages(1, 'FG_PCT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec85cf0-cae7-443e-9014-f409b70130af",
   "metadata": {},
   "source": [
    "Above we can see that field goal percentage is always significantly higher when games are won. Thus, when we expect a team to have a higher field goal percentage, they have a higher likelihood of winning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89436d9-ca4e-49aa-b927-437d58bb4459",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca983ca-2938-4fe0-b88b-3e0280ebe2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d0f7c-13b5-46e4-b97a-6cde4b4585b5",
   "metadata": {},
   "source": [
    "Features to add : \n",
    "1) Win streak\n",
    "2) Win percentage\n",
    "3) ELO Scores\n",
    "4) EFG%\n",
    "5) TS%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a20f7-6604-4c09-9106-e24b5e23849a",
   "metadata": {},
   "source": [
    "### Win Streak and Win Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54849da8-424f-4e25-aaf5-d9ff3ccc30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_win_streak_and_percentage(df, combined=False):\n",
    "    \"\"\"\n",
    "    Input: Dataframe with team one and team two data for each game and boolean to check if dataframe is combined with both team data\n",
    "    Output: New dataframe with added win streak and win percentage for both teams\n",
    "    \"\"\"\n",
    "    team_date_stats = all_stats_cleaned[['TEAM_ID', 'GAME_DATE', 'WIN']].sort_values(by=['TEAM_ID', 'GAME_DATE']).reset_index(drop=True)\n",
    "    team_date_stats['WIN_STREAK'] = 0\n",
    "    team_date_stats['WIN_PERCENTAGE'] = 0.0\n",
    "    \n",
    "    for team_id, group in team_date_stats.groupby('TEAM_ID'):\n",
    "        streak = 0\n",
    "        wins = 0\n",
    "        total_games = 0\n",
    "        indices = group.index\n",
    "    \n",
    "        for i in range(len(indices)):\n",
    "            idx = indices[i]\n",
    "    \n",
    "            # WIN STREAK\n",
    "            team_date_stats.at[idx, 'WIN_STREAK'] = streak\n",
    "    \n",
    "            if team_date_stats.at[idx, 'WIN'] == 1:\n",
    "                streak += 1\n",
    "            else: \n",
    "                streak = 0\n",
    "    \n",
    "            # WIN PERCENTAGE\n",
    "            if total_games == 0:\n",
    "                team_date_stats.at[idx, 'WIN_PERCENTAGE'] = 0.0\n",
    "            else: \n",
    "                team_date_stats.at[idx, 'WIN_PERCENTAGE'] = wins / total_games\n",
    "    \n",
    "            total_games += 1\n",
    "            if team_date_stats.at[idx, 'WIN'] == 1:\n",
    "                wins += 1\n",
    "\n",
    "    if combined:\n",
    "    # Join Win streak and Win percentage of team one and team two into the merged table\n",
    "        team_date_stats.drop('WIN', axis=1, inplace=True)\n",
    "        df = pd.merge(df, team_date_stats,\n",
    "                              how='left', \n",
    "                              left_on = ['TEAM_ID_ONE', 'GAME_DATE'],\n",
    "                              right_on=['TEAM_ID', 'GAME_DATE'])\n",
    "        df.drop('TEAM_ID', axis=1, inplace=True)\n",
    "        df.rename(columns = {'WIN_STREAK': 'WIN_STREAK_ONE',\n",
    "                                     'WIN_PERCENTAGE': 'WIN_PERCENTAGE_ONE'}, inplace=True)\n",
    "        df = pd.merge(df, team_date_stats,\n",
    "                              how='left', \n",
    "                              left_on = ['TEAM_ID_TWO', 'GAME_DATE'],\n",
    "                              right_on=['TEAM_ID', 'GAME_DATE'])\n",
    "        df.drop('TEAM_ID', axis=1, inplace=True)\n",
    "        df.rename(columns = {'WIN_STREAK': 'WIN_STREAK_TWO',\n",
    "                                     'WIN_PERCENTAGE': 'WIN_PERCENTAGE_TWO'}, inplace=True)\n",
    "    else:\n",
    "        # Join Win streak and Win percentage into the dataframe\n",
    "        team_date_stats.drop('WIN', axis=1, inplace=True)\n",
    "        df = pd.merge(df, team_date_stats,\n",
    "                              how='left', \n",
    "                              on = ['TEAM_ID', 'GAME_DATE'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e014a30-619f-4b87-b757-e444d91b5a9e",
   "metadata": {},
   "source": [
    "### ELO Score Before Current Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835ead9-c09c-4961-9e78-35c7213cb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_opponent_points(df):\n",
    "    df_opp = df[['TEAM_ABBREVIATION', 'GAME_DATE', 'PTS', 'TEAM_ID']].copy()\n",
    "    merged_df = pd.merge(df, df_opp, \n",
    "                         how='left',\n",
    "                          left_on=['GAME_DATE', 'OPPONENT'],\n",
    "                            right_on=['GAME_DATE', 'TEAM_ABBREVIATION'],\n",
    "                          suffixes=('', '_OPPONENT'))\n",
    "    merged_df.drop(columns=['TEAM_ABBREVIATION_OPPONENT'], inplace=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865af94b-e088-4e1b-bf03-a0029008eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elo_score(df, combined=False):\n",
    "    \"\"\"\n",
    "    Input: Dataframe with team one and team two data for each game and boolean to check if dataframe is combined with both team data\n",
    "    Output: New dataframe with elo scores for both teams added \n",
    "    \"\"\"\n",
    "    if combined:\n",
    "        df['GAME_ID'] = df.apply(\n",
    "        lambda row: '_'.join(sorted([str(row['TEAM_ID_ONE']), str(row['TEAM_ID_TWO'])]) + [str(row['GAME_DATE'])]),\n",
    "        axis=1\n",
    "    )\n",
    "        df['ELO_ONE'] = np.nan\n",
    "        df['ELO_TWO'] = np.nan\n",
    "    else:\n",
    "        df = merge_opponent_points(df)\n",
    "        df['ELO'] = np.nan\n",
    "        df['GAME_ID'] = df.apply(\n",
    "        lambda row: '_'.join(sorted([str(row['TEAM_ID']), str(row['TEAM_ID_OPPONENT'])]) + [str(row['GAME_DATE'])]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    team_elos = {} # to use for checking if a team has appeared and track team last elo scores\n",
    "    team_last_season = {} # to track last seasons of teams\n",
    "    processed_games = set() # to track game id - handle duplicate game columns\n",
    "    elo_map = {} # for faster computation\n",
    "    df = df.sort_values(by='GAME_DATE').reset_index(drop=True)\n",
    "    \n",
    "    for i,row in df.iterrows():\n",
    "        season = row['SEASON_YEAR']\n",
    "        game_id = row['GAME_ID']\n",
    "\n",
    "        if game_id in processed_games:\n",
    "            continue\n",
    "        processed_games.add(game_id)\n",
    "\n",
    "        if combined:\n",
    "            team_one, team_two = row['TEAM_ID_ONE'], row['TEAM_ID_TWO']\n",
    "            points_one, points_two = row['PTS_ONE'], row['PTS_TWO']\n",
    "            home_one = row['HOME_ONE']\n",
    "        \n",
    "            # Season adjustment formula for ELO : New Season ELO = 0.75 * Last Season ELO + 0.25 * Mean ELO, Mean ELO = 1505\n",
    "            for team in [team_one, team_two]:\n",
    "                # check if team has not appeared yet in the dataset\n",
    "                if team not in team_elos:\n",
    "                    team_elos[team] = 1505 \n",
    "                    team_last_season[team] = season\n",
    "                # check for new season, if yes, apply season adjustment\n",
    "                elif team_last_season[team] != season:\n",
    "                    team_elos[team] = 0.75 * team_elos[team] + 0.25 * 1505\n",
    "                    team_last_season[team] = season\n",
    "        \n",
    "            # elo scores before game\n",
    "            elo_one = team_elos[team_one]\n",
    "            elo_two = team_elos[team_two]\n",
    "        \n",
    "            # Add 100 score to home team\n",
    "            if home_one == 1:\n",
    "                elo_one_after_home_adv = elo_one + 100 \n",
    "                elo_two_after_home_adv = elo_two\n",
    "            else:\n",
    "                elo_one_after_home_adv = elo_one \n",
    "                elo_two_after_home_adv = elo_two + 100\n",
    "        \n",
    "            # Expected score of game formula : exp = 1/ (1+10^((ELO two after home advantage - ELO one after home advantage) / 400))\n",
    "            exp = 1/ (1+10**((elo_two_after_home_adv - elo_one_after_home_adv) / 400))\n",
    "        \n",
    "            actual = 1 if points_one > points_two else 0\n",
    "            margin_of_victory = abs(points_one - points_two)\n",
    "        \n",
    "            # Margin of Victory Multiplier formula : ((MOV + 3) ** 0.8) / (7.5 + 0.006 * (Elo team one - Elo team two))\n",
    "            MOVM = ((margin_of_victory + 3) ** 0.8) / (7.5 + 0.006 * (elo_one - elo_two))\n",
    "        \n",
    "            # change in ELO: K * MOVM * (actual - exp), k -> attenuation factor -> higher means elo score adjusts quickly to changes in strength of team\n",
    "            K = 20 # 20 is optimal for nba \n",
    "            change = K * MOVM * (actual - exp)\n",
    "    \n",
    "            # Update data for ELO ratings\n",
    "            team_elos[team_one] += change\n",
    "            team_elos[team_two] -= change\n",
    "        \n",
    "            # store elo score for game id at the table\n",
    "            # df.at[i, 'ELO_ONE'] = elo_one\n",
    "            # df.at[i, 'ELO_TWO'] = elo_two\n",
    "            # df.loc[(df['GAME_ID'] == game_id) & df['TEAM_ID_ONE'] == team_two, 'ELO_ONE'] = elo_two\n",
    "            # df.loc[(df['GAME_ID'] == game_id) & df['TEAM_ID_TWO'] == team_one, 'ELO_TWO'] = elo_one\n",
    "\n",
    "            # store elo scores in dictionary\n",
    "            elo_map[(game_id, team_one, team_two)] = elo_one\n",
    "            elo_map[(game_id, team_two, team_one)] = elo_two\n",
    "     \n",
    "        else:\n",
    "            team, team_opp = row['TEAM_ID'], row['TEAM_ID_OPPONENT']\n",
    "            points_team, points_opp = row['PTS'], row['PTS_OPPONENT']\n",
    "            home = row['HOME']\n",
    "        \n",
    "            # Season adjustment formula for ELO : New Season ELO = 0.75 * Last Season ELO + 0.25 * Mean ELO, Mean ELO = 1505\n",
    "            for t in [team, team_opp]:\n",
    "                # check if team has not appeared yet in the dataset\n",
    "                if t not in team_elos:\n",
    "                    team_elos[t] = 1505 \n",
    "                    team_last_season[t] = season\n",
    "                # check for new season, if yes, apply season adjustment\n",
    "                elif team_last_season[t] != season:\n",
    "                    team_elos[t] = 0.75 * team_elos[t] + 0.25 * 1505\n",
    "                    team_last_season[t] = season\n",
    "        \n",
    "            # elo scores before game\n",
    "            elo_team = team_elos[team]\n",
    "            elo_opponent = team_elos[team_opp]\n",
    "        \n",
    "            # Add 100 score to home team\n",
    "            if home == 1:\n",
    "                elo_team_home = elo_team + 100 \n",
    "                elo_opp_home = elo_opponent\n",
    "            else:\n",
    "                elo_team_home = elo_team \n",
    "                elo_opp_home = elo_opponent + 100\n",
    "        \n",
    "            # Expected score of game formula : exp = 1/ (1+10^((ELO two after home advantage - ELO one after home advantage) / 400))\n",
    "            exp = 1/ (1+10**((elo_opp_home - elo_team_home) / 400))\n",
    "        \n",
    "            actual = 1 if points_team > points_opp else 0\n",
    "            margin_of_victory = abs(points_team - points_opp)\n",
    "        \n",
    "            # Margin of Victory Multiplier formula : ((MOV + 3) ** 0.8) / (7.5 + 0.006 * (Elo team one - Elo team two))\n",
    "            MOVM = ((margin_of_victory + 3) ** 0.8) / (7.5 + 0.006 * (elo_team - elo_opponent))\n",
    "        \n",
    "            # change in ELO: K * MOVM * (actual - exp), k -> attenuation factor -> higher means elo score adjusts quickly to changes in strength of team\n",
    "            K = 20 # 20 is optimal for nba \n",
    "            change = K * MOVM * (actual - exp)\n",
    "\n",
    "            # Update data for ELO ratings\n",
    "            team_elos[team] += change\n",
    "            team_elos[team_opp] -= change\n",
    "        \n",
    "            # store elo score for both row of game at the table\n",
    "            # df.at[i, 'ELO'] = elo_team\n",
    "            # df.loc[(df['GAME_ID'] == game_id) & df['TEAM_ID'] == team_opp, 'ELO'] = elo_opponent\n",
    "            elo_map[(game_id, team)] = elo_team\n",
    "            elo_map[(game_id, team_opp)] = elo_opponent\n",
    "\n",
    "    # add data from elo dictionary into dataframe\n",
    "    if not combined:\n",
    "        df['ELO'] = df.apply(lambda x: elo_map.get((x['GAME_ID'], x['TEAM_ID']), np.nan), axis=1)\n",
    "        df.drop(columns=['PTS_OPPONENT', 'TEAM_ID_OPPONENT'], axis=1, inplace=True)\n",
    "    else: \n",
    "        df['ELO_ONE'] = df.apply(lambda x: elo_map.get((x['GAME_ID'], x['TEAM_ID_ONE'], x['TEAM_ID_TWO']), np.nan), axis=1)\n",
    "        df['ELO_TWO'] = df.apply(lambda x: elo_map.get((x['GAME_ID'], x['TEAM_ID_TWO'], x['TEAM_ID_ONE']), np.nan), axis=1)\n",
    "    df.drop(columns=['GAME_ID'], axis=1, inplace=True)\n",
    "    \n",
    "            \n",
    "    return df                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79ba04-8bdd-4a9e-9ee7-3b08aca53fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for single team data\n",
    "test_1 = add_win_streak_and_percentage(all_stats_cleaned)\n",
    "test_1 = add_elo_score(test_1)\n",
    "print(test_1.columns)\n",
    "test_1[test_1['TEAM_ID'] == 14].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d55a3-2551-4a4d-9951-4fb1f40aa6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for combined team and opponent data\n",
    "test_2 = add_win_streak_and_percentage(combined_stats, True)\n",
    "test_2 = add_elo_score(test_2, True)\n",
    "test_2[test_2['TEAM_ID_ONE'] == 14].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d137f-3bc0-43e8-879b-c9bc2d9f6c2e",
   "metadata": {},
   "source": [
    "### Effective Field Goal Percentage and True Shooting Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c4746-bd69-4cb4-94fe-e60c5305b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shooting_percentages(df, combined=False):\n",
    "    if combined: \n",
    "        df['EFG%_ONE'] = (df['FGM_ONE'] + 1.5 * df['FG3M_ONE']) / df['FGA_ONE']\n",
    "        df['EFG%_TWO'] = (df['FGM_TWO'] + 1.5 * df['FG3M_TWO']) / df['FGA_TWO']\n",
    "        df['TS%_ONE'] = df['PTS_ONE'] / (2 * (df['FGA_ONE'] + 0.44 * df['FTA_ONE']))\n",
    "        df['TS%_TWO'] = df['PTS_TWO'] / (2 * (df['FGA_TWO'] + 0.44 * df['FTA_TWO']))\n",
    "    else:\n",
    "        df['EFG%'] = (df['FGM'] + 1.5 * df['FG3M']) / df['FGA']\n",
    "        df['TS%'] = df['PTS'] / (2 * (df['FGA'] + 0.44 * df['FTA']))\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00c976-ab01-41a6-b666-66e189e26c6b",
   "metadata": {},
   "source": [
    "### Point Differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763f10a-955f-43d1-b952-33eaecf5f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_point_differential(df, combined=False):\n",
    "        #  add opponent points to all_stats_cleaned table\n",
    "    if combined:\n",
    "        df['PTS_DIFF_ONE'] = df['PTS_ONE'] - df['PTS_TWO']\n",
    "        df['PTS_DIFF_TWO'] = df['PTS_TWO'] - df['PTS_ONE']\n",
    "    else:\n",
    "        df = merge_opponent_points(df)\n",
    "        df['PTS_DIFF'] = df['PTS'] - df['PTS_OPPONENT']\n",
    "        df.drop(columns=['PTS_OPPONENT', 'TEAM_ID_OPPONENT'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f693e-e6da-44e5-8a60-28bf24d618d8",
   "metadata": {},
   "source": [
    "### Win for last matchup game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5764ba-ac0d-4209-9965-470f0ebd5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_win_last_game(df, combined=False):\n",
    "    if combined:\n",
    "        sorted_df = df.sort_values(by=['TEAM_ID_ONE', 'TEAM_ID_TWO', 'GAME_DATE'])\n",
    "        sorted_df['WIN_LAST'] = sorted_df.groupby(['TEAM_ID_ONE', 'TEAM_ID_TWO'])['WIN_ONE'].shift(1)\n",
    "        df = df.merge(sorted_df[['TEAM_ID_ONE', 'TEAM_ID_TWO', 'GAME_DATE', 'WIN_LAST']],\n",
    "                      on=['TEAM_ID_ONE', 'TEAM_ID_TWO', 'GAME_DATE'],\n",
    "                      how = 'left')\n",
    "    else:\n",
    "        sorted_df = df.sort_values(by=['TEAM_ID', 'OPPONENT', 'GAME_DATE'])\n",
    "        sorted_df['WIN_LAST'] = sorted_df.groupby(['TEAM_ID', 'OPPONENT'])['WIN'].shift(1)\n",
    "        df = df.merge(sorted_df[['TEAM_ID', 'OPPONENT', 'GAME_DATE', 'WIN_LAST']],\n",
    "                      on=['TEAM_ID', 'OPPONENT', 'GAME_DATE'],\n",
    "                      how = 'left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917da54-7a54-41ba-8c7a-fb56695c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_4 = add_win_last_game(all_stats_cleaned)\n",
    "print(test_4.columns)\n",
    "test_4[(test_4['TEAM_ID'] == 14) & (test_4['OPPONENT'] == 'BOS')].sort_values(by='GAME_DATE').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f63bd-ac90-4416-9ce4-116cc772b48e",
   "metadata": {},
   "source": [
    "### Rolling Window Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cbd09-4700-4e83-9424-e2cbba02a41b",
   "metadata": {},
   "source": [
    "#### Adding additional columns to all_stats_cleaned -- READ HERE PLS\n",
    "I had to change the function because I was working with all_stats_cleaned which is before we combine and doesn't have _one, _two. Also, I wasn't able to add ELO or win streaks since those functions were specifically tailored to the combined statistics and I was lazy. I think our best bet is to calculate all these values for the all_stats_cleaned dataframe, then create the combined dataset. The ELO and win streak scores do not need to be predicted using a rolling average, so if you just calculate those and add them to the all_stats_cleaned then my code should automatically include them. If you add some other value that should be predicted then either ask me or if my code makes sense, just add to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e48fa-aa5b-46c1-8dd5-0ad3546a43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added shooting percentage\n",
    "all_stats_cleaned = add_shooting_percentages(all_stats_cleaned)\n",
    "# added win streak and win percentage\n",
    "all_stats_cleaned = add_win_streak_and_percentage(all_stats_cleaned)\n",
    "# added ELO score\n",
    "all_stats_cleaned = add_elo_score(all_stats_cleaned)\n",
    "# added point differential\n",
    "# all_stats_cleaned = add_point_differential(all_stats_cleaned)\n",
    "# added win for last game\n",
    "all_stats_cleaned = add_win_last_game(all_stats_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593ec6f-a148-48f7-b768-f13759e6949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['FG_PCT', 'FG3_PCT', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'EFG%', 'TS%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59640b54-e890-4dd3-b9bc-d0f1e1bb426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(n) :\n",
    "    cols = ['FG_PCT', 'FG3_PCT', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'EFG%', 'TS%']\n",
    "    result = None\n",
    "    for team_id in all_stats_cleaned['TEAM_ID'].unique() :\n",
    "        team_data = all_stats_cleaned[all_stats_cleaned['TEAM_ID'] == team_id].sort_values(by='GAME_DATE')\n",
    "        for col in cols :\n",
    "            shift = team_data[col].shift(1)\n",
    "            team_data[col] = shift.rolling(window = n).mean()\n",
    "        if result is None :\n",
    "            result = team_data\n",
    "        else :\n",
    "            result = pd.concat([result, team_data])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232938ad-a747-41a8-ba90-e44603653335",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stats = rolling_window(5)\n",
    "print(pred_stats.shape)\n",
    "pred_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8daffde-15f8-4c50-a10a-cbf607a1ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = pred_stats[pred_stats['HOME'] == 1]\n",
    "away = pred_stats[pred_stats['HOME'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447d9c2-88dc-4cb5-a7d9-cbc78ffda297",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pred_stats_home = pd.merge(home, away, \n",
    "                          left_on=['GAME_DATE', 'OPPONENT'], \n",
    "                          right_on=['GAME_DATE', 'TEAM_ABBREVIATION'],\n",
    "                          suffixes=('_ONE', '_TWO'))\n",
    "combined_pred_stats_away = pd.merge(away, home, \n",
    "                          left_on=['GAME_DATE', 'OPPONENT'], \n",
    "                          right_on=['GAME_DATE', 'TEAM_ABBREVIATION'],\n",
    "                          suffixes=('_ONE', '_TWO'))\n",
    "\n",
    "combined_pred_stats = pd.concat([combined_pred_stats_home, combined_pred_stats_away], ignore_index = True)\n",
    "\n",
    "combined_pred_stats = combined_pred_stats.drop(columns = ['MIN_TWO', 'OPPONENT_ONE', 'OPPONENT_TWO', 'SEASON_YEAR_ONE'])\n",
    "combined_pred_stats.rename(columns={'MIN_ONE': 'MIN', 'SEASON_YEAR_TWO': 'SEASON_YEAR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498888db-65f5-4318-9536-3a113581bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pred_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a8234-8cee-4b10-89f2-5f91d23f0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first n*30 columns which have nan values because rolling window\n",
    "combined_pred_stats = combined_pred_stats.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a0eec-7ba0-4274-85c6-c4455c7b5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pred_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deef007-5859-40a0-8fb3-a6b6b093dc1f",
   "metadata": {},
   "source": [
    "## Predicting Game Stats Using Another Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626f1fc-42c9-4ba6-867f-af334043bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df93f95-71e5-4943-8d16-873f8fac69bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding shooting percentages to the game stats that need to be predicted\n",
    "combined_stats = add_shooting_percentages(combined_stats, combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66648b-3aa3-4dc5-8b60-8f4a94998fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e84f6-9a5a-46c2-947f-5885e9663d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_columns = ['FG_PCT_ONE', 'FG3_PCT_ONE', 'FT_PCT_ONE', 'OREB_ONE', 'DREB_ONE', 'REB_ONE',\n",
    "                    'AST_ONE', 'STL_ONE', 'BLK_ONE', 'TOV_ONE', 'PF_ONE', 'EFG%_ONE', 'TS%_ONE']\n",
    "\n",
    "def get_rolling(team_id, games, prefix, rolling_window = 10):\n",
    "    \"\"\"\n",
    "    Gets rolling statistics for given team using given rolling window.\n",
    "    \"\"\"\n",
    "    stats = {stat: [] for stat in stat_columns}\n",
    "    for _, g in games.tail(rolling_window).iterrows():\n",
    "        if g['TEAM_ID_ONE'] == team_id:\n",
    "            for stat in stat_columns:\n",
    "                stats[stat].append(g[stat])\n",
    "        else:\n",
    "            for stat in stat_columns:\n",
    "                stats[stat].append(g[stat.replace('_ONE', '_TWO')])\n",
    "    return {f\"{prefix}_{stat}\": np.mean(vals) for stat, vals in stats.items()}\n",
    "\n",
    "def build_features_for_game(team_one_id, team_two_id, home_one, date, combined_stats, stat_columns, rolling_window):\n",
    "    \"\"\"\n",
    "    Builds feature vector for a single game using rolling averages and metadata.\n",
    "    \"\"\"\n",
    "    past_games = combined_stats[combined_stats['GAME_DATE'] < date].sort_values('GAME_DATE')\n",
    "\n",
    "    # getting past games for both teams and calculating rolling averages\n",
    "    team_one_games = past_games[\n",
    "        (past_games['TEAM_ID_ONE'] == team_one_id) | (past_games['TEAM_ID_TWO'] == team_one_id)\n",
    "    ]\n",
    "    team_two_games = past_games[\n",
    "        (past_games['TEAM_ID_ONE'] == team_two_id) | (past_games['TEAM_ID_TWO'] == team_two_id)\n",
    "    ]\n",
    "\n",
    "    if len(team_one_games) < rolling_window or len(team_two_games) < rolling_window:\n",
    "        return None\n",
    "\n",
    "    team_one_features = get_rolling(team_one_id, team_one_games, 'TEAM_ONE', rolling_window)\n",
    "    team_two_features = get_rolling(team_two_id, team_two_games, 'TEAM_TWO', rolling_window)\n",
    "\n",
    "    input_features = {\n",
    "        'TEAM_ID_ONE': team_one_id,\n",
    "        'TEAM_ID_TWO': team_two_id,\n",
    "        'HOME_ONE': home_one,\n",
    "        'SEASON_YEAR': date.year if date.month >= 10 else date.year - 1\n",
    "    }\n",
    "    input_features.update(team_one_features)\n",
    "    input_features.update(team_two_features)\n",
    "    return input_features\n",
    "\n",
    "    \n",
    "def train_model(combined_stats, stat_columns, rolling_window = 10):\n",
    "    \"\"\"\n",
    "    Trains a model to predict team stats for a given game using past rolling averages of both teams.\n",
    "    Features: past performance of TEAM_ONE and TEAM_TWO.\n",
    "    Targets: actual stats of TEAM_ONE in the current game.\n",
    "    \"\"\"\n",
    "    combined_stats = combined_stats.sort_values(by='GAME_DATE').reset_index(drop=True)\n",
    "    feature_rows = []\n",
    "    target_dict = {stat: [] for stat in stat_columns}\n",
    "\n",
    "    for idx, row in combined_stats.iterrows():\n",
    "        date = row['GAME_DATE']\n",
    "        team_one = row['TEAM_ID_ONE']\n",
    "        team_two = row['TEAM_ID_TWO']\n",
    "        home = row['HOME_ONE']\n",
    "\n",
    "        features = build_features_for_game(team_one, team_two, home, date, combined_stats, stat_columns, rolling_window)\n",
    "        if features is None:\n",
    "            continue\n",
    "            \n",
    "        feature_rows.append(features)\n",
    "\n",
    "        for stat in stat_columns:\n",
    "            target_dict[stat].append(row[stat])\n",
    "\n",
    "    X = pd.DataFrame(feature_rows)\n",
    "    X = pd.get_dummies(X, columns=['TEAM_ID_ONE', 'TEAM_ID_TWO', 'HOME_ONE'], drop_first=True)\n",
    "\n",
    "    # fitting a XGBoost model for each stat\n",
    "    models = {}\n",
    "    for stat in stat_columns:\n",
    "        y = pd.Series(target_dict[stat])\n",
    "        model = XGBRegressor(n_estimators = 100)\n",
    "        model.fit(X, y)\n",
    "        models[stat] = model\n",
    "\n",
    "    return models, X.columns.tolist()\n",
    "\n",
    "def predict_game_stats(models, feature_cols, combined_stats, team_one_id, team_two_id, home_one, date,\n",
    "                       stat_columns, rolling_window = 10):\n",
    "    \"\"\"\n",
    "    Predicts the statistics for given game.\n",
    "    \"\"\"\n",
    "    features = build_features_for_game(team_one_id, team_two_id, home_one, date, combined_stats, stat_columns, rolling_window)\n",
    "    if features is None:\n",
    "        raise ValueError(\"Not enough past games to make prediction.\")\n",
    "\n",
    "    X_new = pd.DataFrame([features])\n",
    "    X_new = pd.get_dummies(X_new)\n",
    "    for col in feature_cols:\n",
    "        if col not in X_new.columns:\n",
    "            X_new[col] = 0\n",
    "    X_new = X_new[feature_cols]\n",
    "\n",
    "    # predict stats using previously fitted models\n",
    "    prediction = {}\n",
    "    for stat, model in models.items():\n",
    "        prediction[stat] = model.predict(X_new)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d85c94-fc20-4b50-bf64-76848b2865b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stats_model(combined_stats, rolling_window = 10, test_seasons = 4, models = None, feature_cols = None):\n",
    "    \"\"\"\n",
    "    Evaluates predicting stats model by testing on last `test_seasons` seasons using RMSE.\n",
    "    \"\"\"\n",
    "    combined_stats = combined_stats.sort_values('GAME_DATE').reset_index(drop=True)\n",
    "    combined_stats['SEASON_YEAR'] = combined_stats['GAME_DATE'].apply(\n",
    "        lambda d: d.year if d.month >= 10 else d.year - 1\n",
    "    )\n",
    "\n",
    "    stat_columns = ['FG_PCT_ONE', 'FG3_PCT_ONE', 'FT_PCT_ONE', 'OREB_ONE', 'DREB_ONE', 'REB_ONE',\n",
    "                    'AST_ONE', 'STL_ONE', 'BLK_ONE', 'TOV_ONE', 'PF_ONE', 'EFG%_ONE', 'TS%_ONE']\n",
    "\n",
    "    # splitting into training and testing sets\n",
    "    all_seasons = sorted(combined_stats['SEASON_YEAR'].unique())\n",
    "    train_seasons = all_seasons[:-test_seasons]\n",
    "    test_seasons_list = all_seasons[-test_seasons:]\n",
    "    train_data = combined_stats[combined_stats['SEASON_YEAR'].isin(train_seasons)].copy()\n",
    "    test_data = combined_stats[combined_stats['SEASON_YEAR'].isin(test_seasons_list)].copy()\n",
    "\n",
    "    # if models and feature columns not given as parameters\n",
    "    if models is None and feature_cols is None:\n",
    "        models, feature_cols = train_model(train_data, stat_columns, rolling_window)\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    # predict stats for all games in testing set\n",
    "    for _, row in test_data.iterrows():\n",
    "        team_one_id = row['TEAM_ID_ONE']\n",
    "        team_two_id = row['TEAM_ID_TWO']\n",
    "        home = row['HOME_ONE']\n",
    "        date = row['GAME_DATE']\n",
    "\n",
    "        pred = predict_game_stats(\n",
    "                models, feature_cols, combined_stats,\n",
    "                team_one_id = team_one_id,\n",
    "                team_two_id = team_two_id,\n",
    "                home_one = home,\n",
    "                date = date,\n",
    "                stat_columns = stat_columns,\n",
    "                rolling_window = rolling_window\n",
    "            )\n",
    "\n",
    "        predictions.append([pred[stat] for stat in stat_columns])\n",
    "        actuals.append([row[stat] for stat in stat_columns])\n",
    "\n",
    "    # evaluating model's predictions\n",
    "    y_true = np.array(actuals)\n",
    "    y_pred = np.array(predictions)\n",
    "    total_rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    return total_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426780d5-531f-4f00-9311-33a0376d8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluate_stats_model(combined_stats, rolling_window = 10, test_seasons = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc97e59-3df7-461a-ae5e-dbd999d9a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c890aa-a66f-41aa-92f0-8bddb088e6a5",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (NEEDS TO BE FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cf6d7-68bf-4f5b-8e4d-2cc266c28034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41069b5f-c3fd-4cc1-be67-962dbae294b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_stats_model(X, y, n_iter = 5, cv = 3):\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "        'subsample': [0.5, 0.5, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor()\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator = model,\n",
    "        param_distributions = param_grid,\n",
    "        n_iter = n_iter,\n",
    "        cv = cv,\n",
    "        scoring = 'neg_root_mean_squared_error',\n",
    "        verbose = False,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    return search.best_estimator_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1202f-da1c-4069-b6cf-914ae49f82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mult_models(combined_stats, stat_columns, rolling_window = 10, n_iter = 5, cv = 3):\n",
    "    \"\"\"\n",
    "    Trains a model to predict team stats for a given game and does randomized hyperparameter tuning for each stat.\n",
    "    \"\"\"\n",
    "    combined_stats = combined_stats.sort_values(by='GAME_DATE').reset_index(drop=True)\n",
    "    feature_rows = []\n",
    "    target_dict = {stat: [] for stat in stat_columns}\n",
    "\n",
    "    for idx, row in combined_stats.iterrows():\n",
    "        date = row['GAME_DATE']\n",
    "        team_one = row['TEAM_ID_ONE']\n",
    "        team_two = row['TEAM_ID_TWO']\n",
    "        home = row['HOME_ONE']\n",
    "\n",
    "        features = build_features_for_game(team_one, team_two, home, date, combined_stats, stat_columns, rolling_window)\n",
    "        if features is None:\n",
    "            continue\n",
    "            \n",
    "        feature_rows.append(features)\n",
    "\n",
    "        for stat in stat_columns:\n",
    "            target_dict[stat].append(row[stat])\n",
    "\n",
    "    X = pd.DataFrame(feature_rows)\n",
    "    X = pd.get_dummies(X, columns=['TEAM_ID_ONE', 'TEAM_ID_TWO', 'HOME_ONE'], drop_first=True)\n",
    "\n",
    "    # finding best hyperparameters for each stat\n",
    "    models = {}\n",
    "    best_params = {}\n",
    "    \n",
    "    for stat in stat_columns:\n",
    "        print(f\"Tuning model for {stat}...\")\n",
    "        y = pd.Series(target_dict[stat])\n",
    "        model, params = tune_stats_model(X, y, n_iter = 5, cv = 3)\n",
    "        models[stat] = model\n",
    "        best_params[stat] = params\n",
    "\n",
    "    return models, X.columns.tolist(), best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416bb92-c0be-4b7a-a0c1-a1585f7c89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models, feature_cols, best_params = train_mult_models(\n",
    "    combined_stats,\n",
    "    stat_columns = stat_columns,\n",
    "    rolling_window = 5,\n",
    "    n_iter = 5,\n",
    "    cv = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62214c2d-6a89-47b0-82c1-e6212740edde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stat in best_params:\n",
    "    print(f'Best parameters for {stat} :')\n",
    "    for p in best_params[stat]:\n",
    "        print(f'{p}: {best_params[stat][p]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f31df-7462-4509-9527-f2e397ccd0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_tuned = evaluate_stats_model(combined_stats, rolling_window = 10, test_seasons = 4,\n",
    "                                  models = best_models, feature_cols = feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701a792-28cb-498a-96b2-f3d210c6adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165661b8-1f7a-460b-be29-614cf888c706",
   "metadata": {},
   "source": [
    "## Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48d3f2-1a7f-4116-83b3-d93546a2487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860050ac-660c-4f03-bdd6-aae5b66dd45f",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "Since our data follows a time-series format, we are implementing a different method of testing as follows:\n",
    "1. Save the last 4 years of games as a test set.\n",
    "2. Choose a few days from each season, except the last four, for our validation set.\n",
    "3. Run a GridSearchCV for each day of validation games to find best parameters.\n",
    "4. Test the model with best parameters on the test set.\n",
    "\n",
    "We need to retrain the model for each different day we test because we input into the model the number of days since a game occurred as a way to convert the timestamp into a numerical variable that is understandable to the model. \n",
    "\n",
    "For validating and choosing proper parameters, we will test on three days from each season: beginning, after the trade-deadline (middle), and near playoffs. This way, we can see how the model handles different times of the season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26792fad-d522-4a98-8bc9-1328c2f6e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_pred_stats.drop(columns = ['TEAM_ABBREVIATION_ONE', 'TEAM_NAME_ONE', 'MIN', 'FGM_ONE', \n",
    "                                             'FGA_ONE', 'FG3M_ONE', 'FG3A_ONE', 'FTM_ONE', 'FTA_ONE', 'PTS_ONE', \n",
    "                                             'PLUS_MINUS_ONE', 'TEAM_ABBREVIATION_TWO', 'TEAM_NAME_TWO', 'HOME_TWO',\n",
    "                                             'WIN_TWO', 'FGM_TWO', 'FGA_TWO', 'FG3M_TWO', 'FG3A_TWO', 'FTM_TWO', \n",
    "                                             'FTA_TWO', 'PTS_TWO', 'PLUS_MINUS_TWO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a8491-f3f8-4154-bd2c-5d8aeae39fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab7064-8dd7-478d-adec-f8d7a630e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_set (first_season, last_season) :\n",
    "    dates = []\n",
    "    for season in range(first_season, last_season) :\n",
    "        season_data = df[df['SEASON_YEAR'] == season]\n",
    "        start_date = season_data['GAME_DATE'].min()\n",
    "        end_date = season_data['GAME_DATE'].max()\n",
    "\n",
    "        # day around the beginning of the season\n",
    "        beg = season_data[season_data['GAME_DATE'].between(start_date, start_date + timedelta(weeks = 4))]\n",
    "\n",
    "        # day around trade deadline (after about 2/3 of the season)\n",
    "        delta = round((2/3)*(end_date-start_date).days)\n",
    "        approx_deadline = start_date + timedelta(days = delta)\n",
    "        mid = season_data[season_data['GAME_DATE'].between(approx_deadline, approx_deadline + timedelta(weeks = 4))]\n",
    "        \n",
    "        # day around the end of the season\n",
    "        end = season_data[season_data['GAME_DATE'].between(end_date - timedelta(weeks = 4), end_date)]\n",
    "\n",
    "        dates.extend(list(pd.concat([beg.sample(2)['GAME_DATE'], mid.sample(2)['GAME_DATE'], end.sample(2)['GAME_DATE']])))\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efe34a-034c-44dc-9cd1-1193ea78f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_season = df['SEASON_YEAR'].min() + 5\n",
    "last_season = df['SEASON_YEAR'].max() - 4\n",
    "test = get_test_set(first_season, last_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d0197-3028-4b65-b2b4-99a158af3af7",
   "metadata": {},
   "source": [
    "### Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff14a18-705e-4ac2-a905-2236e8193257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set (date, num_seasons) :\n",
    "    \"\"\"\n",
    "    Input: Date of games and number of seasons to include in dataset\n",
    "    Output: All rows from the last num_seasons and all games in the current season up till the given date\n",
    "    \"\"\"\n",
    "    # determine season of the game\n",
    "    season = date.year if date.month >= 10 else date.year - 1\n",
    "    \n",
    "    # get games for training\n",
    "    data = df[df['SEASON_YEAR'].between(season - num_seasons, season)].copy()\n",
    "    data['DAYS_SINCE_GAME'] = [(date-game_day).days for game_day in data['GAME_DATE']]\n",
    "    data = data[data['DAYS_SINCE_GAME'] > 0]\n",
    "\n",
    "    data = data.sort_values(by = 'DAYS_SINCE_GAME')\n",
    "\n",
    "    # split into X and y and only look at relevant columns\n",
    "    X = data.drop(columns = ['WIN_ONE', 'GAME_DATE'])\n",
    "    y = data['WIN_ONE']\n",
    "\n",
    "    return (X,y)\n",
    "\n",
    "def pred_by_date (model, date) :\n",
    "    \"\"\"\n",
    "    Predict the outcome of all games on the given date. \n",
    "    \"\"\"\n",
    "    n = 5 # how many years in the past for training\n",
    "    \n",
    "    # determine season of the game\n",
    "    season = date.year if date.month >= 10 else date.year - 1\n",
    "\n",
    "    # get data in relevant time frame\n",
    "    X, y = get_training_set(date, n)\n",
    "\n",
    "    #df = pd.DataFrame(columns=X.columns)\n",
    "\n",
    "    games_on_day = df[df['GAME_DATE'] == date].copy()\n",
    "    games_on_day['DAYS_SINCE_GAME'] = np.zeros(len(games_on_day))\n",
    "\n",
    "    test = games_on_day.drop(columns = ['WIN_ONE', 'GAME_DATE'])\n",
    "\n",
    "    model.fit(X,y)\n",
    "    pred = model.predict(test)\n",
    "    correct = np.sum(pred == games_on_day['WIN_ONE'])\n",
    "    games = len(pred)\n",
    "    return correct, games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee16dcb-e4a8-4245-b8e4-81e051854767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing the model by choosing 3 days each season and checking score\n",
    "def test_model(model) :\n",
    "    total_correct = total_games = 0\n",
    "\n",
    "    for t in test:\n",
    "        correct, games = pred_by_date(model, t)\n",
    "\n",
    "        total_correct += correct\n",
    "        total_games += games\n",
    "    return total_correct, total_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16080c40-144f-4745-9d9b-d4b429cbb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(objective='binary:logistic', random_state = 33)\n",
    "correct,games = test_model(model)\n",
    "correct / games\n",
    "\n",
    "# before FE : 0.6298076923076923\n",
    "# didn't include point differential since accuracy is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8aebd-1a13-4d39-a7e1-503eec967d90",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609c627-f25c-4a53-bcc4-8e51b475b336",
   "metadata": {},
   "source": [
    "The average feature importance scores is calculated for the three games for each season using XG Boost built-in feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce81c4-0431-49bb-af42-1e3acfc80164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_by_date_with_importance(model, date):\n",
    "    n = 5 \n",
    "    season = date.year if date.month >= 10 else date.year - 1\n",
    "    X, y = get_training_set(date, n)\n",
    "    # one hot encoding on the Home feature \n",
    "    games_on_day = df[df['GAME_DATE'] == date].copy()\n",
    "    games_on_day['DAYS_SINCE_GAME'] = np.zeros(len(games_on_day))\n",
    "\n",
    "    test = games_on_day.drop(columns = ['WIN_ONE', 'GAME_DATE'])\n",
    "\n",
    "    model.fit(X,y)\n",
    "    pred = model.predict(test)\n",
    "    correct = np.sum(pred == games_on_day['WIN_ONE'])\n",
    "    games = len(pred)\n",
    "    importance_scores = model.get_booster().get_score(importance_type='gain')\n",
    "    return correct, games, importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a114ef-e7ea-48f4-9eb9-8df973cb1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_importance(model) :\n",
    "    \"\"\"\n",
    "    Outputs the average feature importance scores of game predictions\n",
    "    \"\"\"\n",
    "    total_correct = total_games = 0\n",
    "    feature_scores = {}\n",
    "    for t in test:\n",
    "        correct, games, importance_scores = pred_by_date_with_importance(model, t)\n",
    "        \n",
    "        for feature, score in importance_scores.items():\n",
    "            if feature not in feature_scores:\n",
    "                feature_scores[feature] = []\n",
    "            feature_scores[feature].append(score)\n",
    "            \n",
    "\n",
    "        total_correct += correct\n",
    "        total_games += games\n",
    "\n",
    "    average_importance = {features: sum(scores)/len(scores) for features, scores in feature_scores.items()}  \n",
    "    sorted_features = sorted(average_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10a8ee-6096-4371-8dd7-b27b2edb2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(objective='binary:logistic')\n",
    "importance_scores = test_model_with_importance(model)\n",
    "print(importance_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a819828-a7f5-49e1-a494-3a41dff88335",
   "metadata": {},
   "source": [
    "Testing the model with the feature importance scores by iteratively removing the least important features and comparing the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef51f5e-aa1b-4105-a3f9-610da7bfe9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set_with_features (date, num_seasons, features) :\n",
    "    \"\"\"\n",
    "    Input: Date of games, number of seasons and feature subset to include in dataset\n",
    "    Output: All rows from the last num_seasons and all games in the current season up till the given date\n",
    "    \"\"\"\n",
    "    season = date.year if date.month >= 10 else date.year - 1\n",
    "    data = df[df['SEASON_YEAR'].between(season - num_seasons, season)].copy()\n",
    "    data['DAYS_SINCE_GAME'] = [(date-game_day).days for game_day in data['GAME_DATE']]\n",
    "    data = data[data['DAYS_SINCE_GAME'] > 0]\n",
    "\n",
    "    data = data.sort_values(by = 'DAYS_SINCE_GAME')\n",
    "\n",
    "    X = data[features]\n",
    "    y = data['WIN_ONE']\n",
    "\n",
    "    return (X,y)\n",
    "\n",
    "def pred_by_date_with_features (model, date, features) :\n",
    "    n = 5 \n",
    "    season = date.year if date.month >= 10 else date.year - 1\n",
    "\n",
    "    X, y = get_training_set_with_features(date, n, features)\n",
    "\n",
    "    games_on_day = df[df['GAME_DATE'] == date].copy()\n",
    "    games_on_day['DAYS_SINCE_GAME'] = np.zeros(len(games_on_day))\n",
    "\n",
    "    test = games_on_day[features]\n",
    "    model.fit(X,y)\n",
    "    pred = model.predict(test)\n",
    "    correct = np.sum(pred == games_on_day['WIN_ONE'])\n",
    "    games = len(pred)\n",
    "    return correct, games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b97d1c-1e87-428a-9bf3-06f9f99a1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_importance(model, current_features, min_subset_size, top_n) :\n",
    "    \"\"\"\n",
    "    Iterates through the feature importance scores and iteratively remove the least importance features\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # current_features = [f[0] for f in feature_importance]\n",
    "    \n",
    "    while len(current_features) >= min_subset_size:\n",
    "        total_correct = total_games = 0\n",
    "        print(f\"Evaluating with {len(current_features)} features...\")\n",
    "        for t in test:    \n",
    "            correct, games = pred_by_date_with_features(model, t, features = current_features)\n",
    "        \n",
    "            total_correct += correct\n",
    "            total_games += games\n",
    "        print(current_features, ':', total_correct/total_games)\n",
    "        results.append((current_features.copy(), total_correct/total_games))\n",
    "        current_features.pop(-1)\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c115eae-4217-49c6-a0a1-67cd4d393b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(objective='binary:logistic')\n",
    "sorted_features = [f[0] for f in importance_scores]\n",
    "print(sorted_features)\n",
    "top_subsets = feature_selection_with_importance(model, sorted_features, min_subset_size=20, top_n=10)\n",
    "\n",
    "for i, (subset, acc) in enumerate(top_subsets, 1):\n",
    "    print(f\"#{i}: Features = {subset}, Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15196ee5-051c-460c-8e03-2a90839ee184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best performing feature subset\n",
    "best_feature_subset = top_subsets[0][0]\n",
    "print('Best feature subset: ', best_feature_subset)\n",
    "total_correct = total_games = 0\n",
    "for t in test:\n",
    "    correct, games = pred_by_date_with_features(model, t, best_feature_subset)\n",
    "\n",
    "    total_correct += correct\n",
    "    total_games += games\n",
    "print('Accuracy:', total_correct / total_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021aa82-c054-4cf9-a75d-822827baf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import combinations\n",
    "# def feature_selection(model, feature_names, min_subset_size, max_subset_size, top_n) :\n",
    "#     \"\"\"\n",
    "#     Iterates through the feature subsets and returns the top n subsets that gives the best scores\n",
    "#     \"\"\"\n",
    "#     print('start')\n",
    "#     results = []\n",
    "#     for n in range(min_subset_size, max_subset_size + 1):\n",
    "#         print(n)\n",
    "#         for subset in combinations(feature_names, n):\n",
    "#             print(subset)\n",
    "#             total_correct = total_games = 0\n",
    "#             for t in test:\n",
    "#                 print('test')\n",
    "#                 correct, games = pred_by_date_with_features(model, t, features = list(subset))\n",
    "        \n",
    "#                 total_correct += correct\n",
    "#                 total_games += games\n",
    "#             print(subset, ':', total_correct/total_games)\n",
    "#             results.append((subset, correct/games))\n",
    "#     results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae0524-0f30-49a1-aa25-581150e0e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes through every combinations of size 40; takes too long (5+ hours)\n",
    "# model = XGBClassifier(objective='binary:logistic')\n",
    "# all_features = [col for col in df.columns if col not in ['WIN_ONE', 'GAME_DATE', 'SEASON_YEAR']]\n",
    "# top_subsets = feature_selection(model, all_features, min_subset_size=40, max_subset_size=40, top_n=10)\n",
    "\n",
    "# for i, (subset, acc, total) in enumerate(top_subsets, 1):\n",
    "#     print(f\"#{i}: Features = {subset}, Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35db0f-dfd9-4878-8913-ff46cb83fb49",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a6dd-8bfe-4f6f-bdeb-909b7bcb9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_by_date_multiple_models (models_dict, date) :\n",
    "    \"\"\"\n",
    "    Predict the outcome of all games on the given date for all models given. Used specifically to make\n",
    "    cross validation more efficient\n",
    "    \"\"\"\n",
    "    n = 5 # how many years in the past for training\n",
    "    \n",
    "    # determine season of the game\n",
    "    season = date.year if date.month >= 10 else date.year - 1\n",
    "\n",
    "    # get data in relevant time frame\n",
    "    X, y = get_training_set(date, n)\n",
    "\n",
    "    games_on_day = df[df['GAME_DATE'] == date].copy()\n",
    "    games_on_day['DAYS_SINCE_GAME'] = np.zeros(len(games_on_day))\n",
    "\n",
    "    test = games_on_day.drop(columns = ['WIN_ONE', 'GAME_DATE'])\n",
    "\n",
    "    scores = np.zeros(len(models_dict))\n",
    "    for k, v in models_dict.items() :\n",
    "        v.fit(X,y)\n",
    "        pred = v.predict(test)\n",
    "        scores[k] = np.sum(pred == games_on_day['WIN_ONE'])\n",
    "    return scores, len(games_on_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024d2de-3105-4756-82cb-a22e0e083d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# XGBoost parameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 400],\n",
    "    \"eta\": [0.01, 0.05, 0.1, 0.2], # learning_rate\n",
    "    \"max_depth\": [4, 6, 8, 10], # maximum depth of a tree\n",
    "    \"subsample\": [0.5, 0.7, 1], # fraction of observation to be radnomly sampled for each tree\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1], # fraction of columns to be random samples for each tree\n",
    "    \"alpha\": [0.5, 1, 2, 5] # lasso regression\n",
    "}\n",
    "\n",
    "param_dict = {} # store params with key corresponding to index of score in np.array\n",
    "index = 0\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for values in itertools.product(*param_grid.values()):\n",
    "    param_dict[index] = XGBClassifier(objective='binary:logistic', random_state = 33, **dict(zip(param_grid.keys(), values)))\n",
    "    index += 1\n",
    "\n",
    "scores = np.zeros(len(param_dict))\n",
    "total_games = 0\n",
    "\n",
    "first_season = df['SEASON_YEAR'].min()\n",
    "last_season = df['SEASON_YEAR'].max()-4\n",
    "\n",
    "for t in test:\n",
    "    s, g = pred_by_date_multiple_models(param_dict, t)\n",
    "\n",
    "    scores += s\n",
    "    total_games += g\n",
    "    print(scores / total_games)\n",
    "\n",
    "print('final scores: ', scores / total_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a27693-7bba-4722-81f5-d49cd64b6a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores = scores / total_games\n",
    "best_model = param_dict[all_scores.argmax()]\n",
    "best_model.get_params() #'n_estimators': 400, eta: 0.01, max_depth: 4, subsample: 0.7, colsample_bytree: 0.7, alpha: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a830931-febe-453a-bd08-44b895bb871a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_five_models = np.argpartition(all_scores, -5)[-5:]\n",
    "top_five_models = top_five_models[np.argsort(-all_scores[top_five_models])]\n",
    "top_five_scores = all_scores[top_five_models]\n",
    "print(top_five_scores)\n",
    "for i in top_five_models : \n",
    "    p = param_dict[i].get_params()\n",
    "    print(f\"n_estimators = {p['n_estimators']}, eta = {p['eta']}, max_depth = {p['max_depth']}, subsample = {p['subsample']}, colsample_bytree = {p['colsample_bytree']}, alpha = {p['alpha']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a4476-ea51-44f9-bb59-70823f83ce8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model = XGBClassifier(n_estimators = 50, eta = 0.05, max_depth = 4, subsample = 0.5, colsample_bytree = 0.5, alpha = 1)\n",
    "correct, games = test_model(final_model)\n",
    "print(\"Score:\", correct / games)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df151b9a",
   "metadata": {},
   "source": [
    "## Predicting Game Stats Using Another Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_columns = ['FG_PCT_ONE', 'FG3_PCT_ONE', 'FT_PCT_ONE', 'OREB_ONE', 'DREB_ONE', 'REB_ONE',\n",
    "                    'AST_ONE', 'STL_ONE', 'BLK_ONE', 'TOV_ONE', 'PF_ONE']\n",
    "\n",
    "def get_rolling(team_id, games, prefix, rolling_window = 10):\n",
    "    \"\"\"\n",
    "    Gets rolling statistics for given team using given rolling window.\n",
    "    \"\"\"\n",
    "    stats = {stat: [] for stat in stat_columns}\n",
    "    for _, g in games.tail(rolling_window).iterrows():\n",
    "        if g['TEAM_ID_ONE'] == team_id:\n",
    "            for stat in stat_columns:\n",
    "                stats[stat].append(g[stat])\n",
    "        else:\n",
    "            for stat in stat_columns:\n",
    "                stats[stat].append(g[stat.replace('_ONE', '_TWO')])\n",
    "    return {f\"{prefix}_{stat}\": np.mean(vals) for stat, vals in stats.items()}\n",
    "\n",
    "def build_features_for_game(team_one_id, team_two_id, home_one, date, combined_stats, stat_columns, rolling_window):\n",
    "    \"\"\"\n",
    "    Builds feature vector for a single game using rolling averages and metadata.\n",
    "    \"\"\"\n",
    "    past_games = combined_stats[combined_stats['GAME_DATE'] < date].sort_values('GAME_DATE')\n",
    "\n",
    "    # getting past games for both teams and calculating rolling averages\n",
    "    team_one_games = past_games[\n",
    "        (past_games['TEAM_ID_ONE'] == team_one_id) | (past_games['TEAM_ID_TWO'] == team_one_id)\n",
    "    ]\n",
    "    team_two_games = past_games[\n",
    "        (past_games['TEAM_ID_ONE'] == team_two_id) | (past_games['TEAM_ID_TWO'] == team_two_id)\n",
    "    ]\n",
    "\n",
    "    if len(team_one_games) < rolling_window or len(team_two_games) < rolling_window:\n",
    "        return None\n",
    "\n",
    "    team_one_features = get_rolling(team_one_id, team_one_games, 'TEAM_ONE', rolling_window)\n",
    "    team_two_features = get_rolling(team_two_id, team_two_games, 'TEAM_TWO', rolling_window)\n",
    "\n",
    "    input_features = {\n",
    "        'TEAM_ID_ONE': team_one_id,\n",
    "        'TEAM_ID_TWO': team_two_id,\n",
    "        'HOME_ONE': home_one,\n",
    "        'SEASON_YEAR': date.year if date.month >= 10 else date.year - 1\n",
    "    }\n",
    "    input_features.update(team_one_features)\n",
    "    input_features.update(team_two_features)\n",
    "    return input_features\n",
    "\n",
    "    \n",
    "def train_model(combined_stats, stat_columns, rolling_window = 10):\n",
    "    \"\"\"\n",
    "    Trains a model to predict team stats for a given game using past rolling averages of both teams.\n",
    "    Features: past performance of TEAM_ONE and TEAM_TWO.\n",
    "    Targets: actual stats of TEAM_ONE in the current game.\n",
    "    \"\"\"\n",
    "    combined_stats = combined_stats.sort_values(by='GAME_DATE').reset_index(drop=True)\n",
    "    feature_rows = []\n",
    "    target_dict = {stat: [] for stat in stat_columns}\n",
    "\n",
    "    for idx, row in combined_stats.iterrows():\n",
    "        date = row['GAME_DATE']\n",
    "        team_one = row['TEAM_ID_ONE']\n",
    "        team_two = row['TEAM_ID_TWO']\n",
    "        home = row['HOME_ONE']\n",
    "\n",
    "        features = build_features_for_game(team_one, team_two, home, date, combined_stats, stat_columns, rolling_window)\n",
    "        if features is None:\n",
    "            continue\n",
    "            \n",
    "        feature_rows.append(features)\n",
    "\n",
    "        for stat in stat_columns:\n",
    "            target_dict[stat].append(row[stat])\n",
    "\n",
    "    X = pd.DataFrame(feature_rows)\n",
    "    X = pd.get_dummies(X, columns=['TEAM_ID_ONE', 'TEAM_ID_TWO', 'HOME_ONE'], drop_first=True)\n",
    "\n",
    "    # fitting a XGBoost model for each stat\n",
    "    models = {}\n",
    "    for stat in stat_columns:\n",
    "        y = pd.Series(target_dict[stat])\n",
    "        model = XGBRegressor(n_estimators = 100)\n",
    "        model.fit(X, y)\n",
    "        models[stat] = model\n",
    "\n",
    "    return models, X.columns.tolist()\n",
    "\n",
    "def predict_game_stats(models, feature_cols, combined_stats, team_one_id, team_two_id, home_one, date,\n",
    "                       stat_columns, rolling_window = 10):\n",
    "    \"\"\"\n",
    "    Predicts the statistics for given game.\n",
    "    \"\"\"\n",
    "    features = build_features_for_game(team_one_id, team_two_id, home_one, date, combined_stats, stat_columns, rolling_window)\n",
    "    if features is None:\n",
    "        raise ValueError(\"Not enough past games to make prediction.\")\n",
    "\n",
    "    X_new = pd.DataFrame([features])\n",
    "    X_new = pd.get_dummies(X_new)\n",
    "    for col in feature_cols:\n",
    "        if col not in X_new.columns:\n",
    "            X_new[col] = 0\n",
    "    X_new = X_new[feature_cols]\n",
    "\n",
    "    # predict stats using previously fitted models\n",
    "    prediction = {}\n",
    "    for stat, model in models.items():\n",
    "        prediction[stat] = model.predict(X_new)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061204eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stats_model(combined_stats, rolling_window = 5, test_seasons = 4, models = None, feature_cols = None):\n",
    "    \"\"\"\n",
    "    Evaluates predicting stats model by testing on last `test_seasons` seasons using RMSE.\n",
    "    \"\"\"\n",
    "    combined_stats = combined_stats.sort_values('GAME_DATE').reset_index(drop=True)\n",
    "    combined_stats['SEASON_YEAR'] = combined_stats['GAME_DATE'].apply(\n",
    "        lambda d: d.year if d.month >= 10 else d.year - 1\n",
    "    )\n",
    "\n",
    "    stat_columns = ['FG_PCT_ONE', 'FG3_PCT_ONE', 'FT_PCT_ONE', 'OREB_ONE', 'DREB_ONE', 'REB_ONE',\n",
    "                    'AST_ONE', 'STL_ONE', 'BLK_ONE', 'TOV_ONE', 'PF_ONE']\n",
    "\n",
    "    # splitting into training and testing sets\n",
    "    all_seasons = sorted(combined_stats['SEASON_YEAR'].unique())\n",
    "    train_seasons = all_seasons[:-test_seasons]\n",
    "    test_seasons_list = all_seasons[-test_seasons:]\n",
    "    train_data = combined_stats[combined_stats['SEASON_YEAR'].isin(train_seasons)].copy()\n",
    "    test_data = combined_stats[combined_stats['SEASON_YEAR'].isin(test_seasons_list)].copy()\n",
    "\n",
    "    # if models and feature columns not given as parameters\n",
    "    if models is None and feature_cols is None:\n",
    "        models, feature_cols = train_model(train_data, stat_columns, rolling_window)\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    # predict stats for all games in testing set\n",
    "    for _, row in test_data.iterrows():\n",
    "        team_one_id = row['TEAM_ID_ONE']\n",
    "        team_two_id = row['TEAM_ID_TWO']\n",
    "        home = row['HOME_ONE']\n",
    "        date = row['GAME_DATE']\n",
    "\n",
    "        pred = predict_game_stats(\n",
    "                models, feature_cols, combined_stats,\n",
    "                team_one_id = team_one_id,\n",
    "                team_two_id = team_two_id,\n",
    "                home_one = home,\n",
    "                date = date,\n",
    "                stat_columns = stat_columns,\n",
    "                rolling_window = rolling_window\n",
    "            )\n",
    "\n",
    "        predictions.append([pred[stat] for stat in stat_columns])\n",
    "        actuals.append([row[stat] for stat in stat_columns])\n",
    "\n",
    "    # evaluating model's predictions\n",
    "    y_true = np.array(actuals)\n",
    "    y_pred = np.array(predictions)\n",
    "    total_rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    return total_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluate_stats_model(combined_stats, rolling_window = 10, test_seasons = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc965209",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e407022",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73497180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_stats_model(X, y, n_iter = 5, cv = 3):\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "        'subsample': [0.5, 0.5, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor()\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator = model,\n",
    "        param_distributions = param_grid,\n",
    "        n_iter = n_iter,\n",
    "        cv = cv,\n",
    "        scoring = 'neg_root_mean_squared_error',\n",
    "        verbose = False,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    return search.best_estimator_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mult_models(combined_stats, stat_columns, rolling_window = 10, n_iter = 5, cv = 3):\n",
    "    \"\"\"\n",
    "    Trains a model to predict team stats for a given game and does randomized hyperparameter tuning for each stat.\n",
    "    \"\"\"\n",
    "    combined_stats = combined_stats.sort_values(by='GAME_DATE').reset_index(drop=True)\n",
    "    feature_rows = []\n",
    "    target_dict = {stat: [] for stat in stat_columns}\n",
    "\n",
    "    for idx, row in combined_stats.iterrows():\n",
    "        date = row['GAME_DATE']\n",
    "        team_one = row['TEAM_ID_ONE']\n",
    "        team_two = row['TEAM_ID_TWO']\n",
    "        home = row['HOME_ONE']\n",
    "\n",
    "        features = build_features_for_game(team_one, team_two, home, date, combined_stats, stat_columns, rolling_window)\n",
    "        if features is None:\n",
    "            continue\n",
    "            \n",
    "        feature_rows.append(features)\n",
    "\n",
    "        for stat in stat_columns:\n",
    "            target_dict[stat].append(row[stat])\n",
    "\n",
    "    X = pd.DataFrame(feature_rows)\n",
    "    X = pd.get_dummies(X, columns=['TEAM_ID_ONE', 'TEAM_ID_TWO', 'HOME_ONE'], drop_first=True)\n",
    "\n",
    "    # finding best hyperparameters for each stat\n",
    "    models = {}\n",
    "    best_params = {}\n",
    "    \n",
    "    for stat in stat_columns:\n",
    "        print(f\"Tuning model for {stat}...\")\n",
    "        y = pd.Series(target_dict[stat])\n",
    "        model, params = tune_stats_model(X, y, n_iter = 5, cv = 3)\n",
    "        models[stat] = model\n",
    "        best_params[stat] = params\n",
    "\n",
    "    return models, X.columns.tolist(), best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f684b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models, feature_cols, best_params = train_mult_models(\n",
    "    combined_stats,\n",
    "    stat_columns = stat_columns,\n",
    "    rolling_window = 5,\n",
    "    n_iter = 5,\n",
    "    cv = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44edb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stat in best_params:\n",
    "    print(f'Best parameters for {stat} :')\n",
    "    for p in best_params[stat]:\n",
    "        print(f'{p}: {best_params[stat][p]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace1245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_tuned = evaluate_stats_model(combined_stats, rolling_window = 10, test_seasons = 4,\n",
    "                                  models = best_models, feature_cols = feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85187bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e691c",
   "metadata": {},
   "source": [
    "# need to be all the way to here pls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879eb06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
